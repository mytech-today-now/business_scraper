# Production Docker Compose Configuration
# Comprehensive production deployment with monitoring, security, and scalability

services:
  # Main Application
  app:
    build:
      context: .
      dockerfile: Dockerfile.production
      args:
        NODE_ENV: production
    container_name: business-scraper-app
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - DATABASE_URL=postgresql://postgres:${POSTGRES_PASSWORD}@postgres:5432/business_scraper
      - REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - RATE_LIMIT_REDIS_URL=redis://:${REDIS_PASSWORD}@redis:6379
      - ENCRYPTION_KEY=${ENCRYPTION_KEY}
      - JWT_SECRET=${JWT_SECRET}
      - SESSION_SECRET=${SESSION_SECRET}
      - ADMIN_USERNAME=${ADMIN_USERNAME}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD}
      - ADMIN_PASSWORD_HASH=${ADMIN_PASSWORD_HASH}
      - ADMIN_PASSWORD_SALT=${ADMIN_PASSWORD_SALT}
      - MONITORING_ENABLED=true
      - LOG_LEVEL=info
      # API Keys for real scraping
      - GOOGLE_MAPS_API_KEY=${GOOGLE_MAPS_API_KEY}
      - GOOGLE_CUSTOM_SEARCH_API_KEY=${GOOGLE_CUSTOM_SEARCH_API_KEY}
      - GOOGLE_SEARCH_ENGINE_ID=${GOOGLE_SEARCH_ENGINE_ID}
      - AZURE_AI_FOUNDRY_API_KEY=${AZURE_AI_FOUNDRY_API_KEY}
      - AZURE_AI_FOUNDRY_ENDPOINT=${AZURE_AI_FOUNDRY_ENDPOINT}
      - AZURE_AI_FOUNDRY_REGION=${AZURE_AI_FOUNDRY_REGION}
      # Production scraping configuration - real scraping only
      - ENABLE_REAL_SCRAPING=true
    depends_on:
      - postgres
      - redis
      - elasticsearch
    volumes:
      - app-logs:/app/logs
      - app-uploads:/app/uploads
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G

  # Database
  postgres:
    image: postgres:15-alpine
    container_name: business-scraper-db
    restart: unless-stopped
    environment:
      - POSTGRES_DB=business_scraper
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./database/init:/docker-entrypoint-initdb.d
    networks:
      - app-network
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d business_scraper"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: business-scraper-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
    networks:
      - app-network
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  # Elasticsearch for logging and search
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: business-scraper-elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    networks:
      - app-network
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: business-scraper-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
      - nginx-logs:/var/log/nginx
    depends_on:
      - app
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Monitoring with Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: business-scraper-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - app-network

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: business-scraper-grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    networks:
      - app-network

  # Node Exporter for system metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: business-scraper-node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - app-network

  # Backup Service (commented out - backup directory not available)
  # backup:
  #   build:
  #     context: ./backup
  #     dockerfile: Dockerfile
  #   container_name: business-scraper-backup
  #   restart: unless-stopped
  #   environment:
  #     - POSTGRES_HOST=postgres
  #     - POSTGRES_DB=business_scraper
  #     - POSTGRES_USER=postgres
  #     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
  #     - BACKUP_SCHEDULE=0 2 * * *  # Daily at 2 AM
  #     - BACKUP_RETENTION_DAYS=30
  #     - S3_BUCKET=${BACKUP_S3_BUCKET}
  #     - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
  #     - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
  #   volumes:
  #     - ./database/backups:/backups
  #     - backup-logs:/var/log/backup
  #   depends_on:
  #     - postgres
  #   networks:
  #     - app-network

  # Log aggregation with Fluentd (commented out - logging directory not available)
  # fluentd:
  #   build:
  #     context: ./logging
  #     dockerfile: Dockerfile
  #   container_name: business-scraper-fluentd
  #   restart: unless-stopped
  #   volumes:
  #     - ./logging/fluent.conf:/fluentd/etc/fluent.conf:ro
  #     - app-logs:/var/log/app:ro
  #     - nginx-logs:/var/log/nginx:ro
  #     - backup-logs:/var/log/backup:ro
  #   ports:
  #     - "24224:24224"
  #     - "24224:24224/udp"
  #   depends_on:
  #     - elasticsearch
  #   networks:
  #     - app-network

  # Security scanner
  security-scanner:
    image: aquasec/trivy:latest
    container_name: business-scraper-security
    restart: "no"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - trivy-cache:/root/.cache/trivy
    command: ["image", "--exit-code", "1", "business-scraper-app:latest"]
    networks:
      - app-network

# Networks
networks:
  app-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# Volumes
volumes:
  postgres-data:
    driver: local
  redis-data:
    driver: local
  elasticsearch-data:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  app-logs:
    driver: local
  app-uploads:
    driver: local
  nginx-logs:
    driver: local
  backup-logs:
    driver: local
  trivy-cache:
    driver: local

# Secrets (use Docker secrets in production)
secrets:
  postgres_password:
    external: true
  redis_password:
    external: true
  encryption_key:
    external: true
  jwt_secret:
    external: true
