# =============================================================================
# DOCKER PRODUCTION COMPOSE CONFIGURATION
# Business Scraper App - Containerized Production Deployment
# =============================================================================
#
# Comprehensive production deployment with monitoring, security, and scalability
# Optimized for Docker environment variables and secrets management
#
# USAGE:
# 1. Ensure .env.production file is configured with production values
# 2. Deploy: docker-compose -f docker-compose.production.yml --env-file .env.production up -d
# 3. Monitor: docker-compose -f docker-compose.production.yml logs -f
#
# =============================================================================



# Docker networks for service isolation
networks:
  app-network:
    driver: ${DOCKER_NETWORK_DRIVER:-bridge}
    ipam:
      config:
        - subnet: ${DOCKER_NETWORK_SUBNET:-172.20.0.0/16}

# Docker volumes for data persistence
volumes:
  postgres-data:
    name: ${POSTGRES_DATA_VOLUME:-postgres-data}
  redis-data:
    name: ${REDIS_DATA_VOLUME:-redis-data}
  elasticsearch-data:
    name: ${ELASTICSEARCH_DATA_VOLUME:-elasticsearch-data}
  app-logs:
    name: ${APP_LOGS_VOLUME:-app-logs}
  app-uploads:
    name: ${APP_UPLOADS_VOLUME:-app-uploads}
  prometheus-data:
    name: ${PROMETHEUS_DATA_VOLUME:-prometheus-data}
  grafana-data:
    name: ${GRAFANA_DATA_VOLUME:-grafana-data}
  trivy-cache:
    name: ${TRIVY_CACHE_VOLUME:-trivy-cache}
  nginx-logs:
    name: ${NGINX_LOGS_VOLUME:-nginx-logs}

services:
  # =============================================================================
  # MAIN APPLICATION SERVICE (COMMENTED OUT FOR LOCAL DEPLOYMENT)
  # =============================================================================
  # app:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.production
  #     args:
  #       NODE_ENV: production
  #       DOCKER_BUILDKIT: ${DOCKER_BUILDKIT:-1}
  #   container_name: ${CONTAINER_NAME_PREFIX:-business-scraper}-app
  #   hostname: business-scraper-app
  #   restart: ${RESTART_POLICY:-unless-stopped}
  #   ports:
  #     - "${PORT:-3000}:${PORT:-3000}"
  #   # Use env_file for cleaner configuration management
  #   env_file:
  #     - .env.production
  #   environment:
  #     # Override critical Docker-specific settings
  #     - NODE_ENV=production
  #     - HOSTNAME=0.0.0.0
  #     - PORT=${PORT:-3000}
  #     - DOCKER_DEPLOYMENT=true
  #   # Resource limits for production
  #   deploy:
  #     resources:
  #       limits:
  #         cpus: ${CONTAINER_CPU_LIMIT:-2.0}
  #         memory: ${CONTAINER_MEMORY_LIMIT:-4G}
  #       reservations:
  #         cpus: ${CONTAINER_CPU_RESERVATION:-1.0}
  #         memory: ${CONTAINER_MEMORY_RESERVATION:-2G}
  #   depends_on:
  #     postgres:
  #       condition: service_healthy
  #     redis:
  #       condition: service_healthy
  #   volumes:
  #     - app-logs:/app/logs
  #     - app-uploads:/app/uploads
  #   networks:
  #     - app-network
  #   healthcheck:
  #     test: ["CMD", "curl", "-f", "http://localhost:${PORT:-3000}${HEALTH_CHECK_ENDPOINT:-/api/health}"]
  #     interval: ${HEALTH_CHECK_INTERVAL:-30s}
  #     timeout: ${HEALTH_CHECK_TIMEOUT:-10s}
  #     retries: ${HEALTH_CHECK_RETRIES:-3}
  #     start_period: ${HEALTH_CHECK_START_PERIOD:-40s}
  #   # Logging configuration for production
  #   logging:
  #     driver: ${DOCKER_LOGGING_DRIVER:-json-file}
  #     options:
  #       max-size: ${DOCKER_LOG_MAX_SIZE:-10m}
  #       max-file: ${DOCKER_LOG_MAX_FILE:-3}

  # Database
  postgres:
    image: postgres:15-alpine
    container_name: business-scraper-db
    restart: unless-stopped
    environment:
      - POSTGRES_DB=business_scraper
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - app-network
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d business_scraper"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Redis Cache
  redis:
    image: redis:7-alpine
    container_name: business-scraper-redis
    restart: unless-stopped
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data
    networks:
      - app-network
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  # Elasticsearch for logging and search (COMMENTED OUT FOR LOCAL DEPLOYMENT)
  # elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: business-scraper-elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    networks:
      - app-network
    ports:
      - "9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    container_name: business-scraper-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - nginx-logs:/var/log/nginx
    depends_on:
      - app
    networks:
      - app-network
    healthcheck:
      test: ["CMD", "nginx", "-t"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Monitoring with Prometheus
  prometheus:
    image: prom/prometheus:latest
    container_name: business-scraper-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    networks:
      - app-network

  # Grafana for visualization
  grafana:
    image: grafana/grafana:latest
    container_name: business-scraper-grafana
    restart: unless-stopped
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - app-network

  # Node Exporter for system metrics
  node-exporter:
    image: prom/node-exporter:latest
    container_name: business-scraper-node-exporter
    restart: unless-stopped
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
    networks:
      - app-network

  # Backup Service (commented out - backup directory not available)
  # backup:
  #   build:
  #     context: ./backup
  #     dockerfile: Dockerfile
  #   container_name: business-scraper-backup
  #   restart: unless-stopped
  #   environment:
  #     - POSTGRES_HOST=postgres
  #     - POSTGRES_DB=business_scraper
  #     - POSTGRES_USER=postgres
  #     - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
  #     - BACKUP_SCHEDULE=0 2 * * *  # Daily at 2 AM
  #     - BACKUP_RETENTION_DAYS=30
  #     - S3_BUCKET=${BACKUP_S3_BUCKET}
  #     - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
  #     - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
  #   volumes:
  #     - ./database/backups:/backups
  #     - backup-logs:/var/log/backup
  #   depends_on:
  #     - postgres
  #   networks:
  #     - app-network

  # Log aggregation with Fluentd (commented out - logging directory not available)
  # fluentd:
  #   build:
  #     context: ./logging
  #     dockerfile: Dockerfile
  #   container_name: business-scraper-fluentd
  #   restart: unless-stopped
  #   volumes:
  #     - ./logging/fluent.conf:/fluentd/etc/fluent.conf:ro
  #     - app-logs:/var/log/app:ro
  #     - nginx-logs:/var/log/nginx:ro
  #     - backup-logs:/var/log/backup:ro
  #   ports:
  #     - "24224:24224"
  #     - "24224:24224/udp"
  #   depends_on:
  #     - elasticsearch
  #   networks:
  #     - app-network

  # Security scanner
  security-scanner:
    image: aquasec/trivy:latest
    container_name: business-scraper-security
    restart: "no"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - trivy-cache:/root/.cache/trivy
    command: ["image", "--exit-code", "1", "business-scraper-app:latest"]
    networks:
      - app-network
